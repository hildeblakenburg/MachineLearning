{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Feb  8 10:54:30 2021\n",
    "Modified Mon Mar 11 10:03 2024\n",
    "@author: batasoy\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as datetime\n",
    "from collections import Counter\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "db = pd.read_csv('modes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>zmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>zmax</th>\n",
       "      <th>xmean</th>\n",
       "      <th>ymean</th>\n",
       "      <th>zmean</th>\n",
       "      <th>xstd</th>\n",
       "      <th>ystd</th>\n",
       "      <th>zstd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a526f3566e9c9024dfa7378eb4291d787a09fd37</td>\n",
       "      <td>2018-04-14 08:39:00</td>\n",
       "      <td>2018-04-14 08:39:05</td>\n",
       "      <td>car</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>-0.177222</td>\n",
       "      <td>2.914167</td>\n",
       "      <td>9.319474</td>\n",
       "      <td>1.823158</td>\n",
       "      <td>3.394375</td>\n",
       "      <td>9.197301</td>\n",
       "      <td>0.680411</td>\n",
       "      <td>3.147691</td>\n",
       "      <td>0.083206</td>\n",
       "      <td>0.848583</td>\n",
       "      <td>0.218015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dd82e3df4bebc74ed6b67877be79e29f401c16a3</td>\n",
       "      <td>2019-05-27 06:11:39</td>\n",
       "      <td>2019-05-27 06:11:44</td>\n",
       "      <td>car</td>\n",
       "      <td>-3.102917</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>9.358878</td>\n",
       "      <td>-2.989388</td>\n",
       "      <td>0.940101</td>\n",
       "      <td>9.446250</td>\n",
       "      <td>-3.034444</td>\n",
       "      <td>0.772117</td>\n",
       "      <td>9.399396</td>\n",
       "      <td>0.047974</td>\n",
       "      <td>0.190513</td>\n",
       "      <td>0.037906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d429974540bfd38c3367fe9f0c8682775ff4fa18</td>\n",
       "      <td>2018-04-19 09:00:56</td>\n",
       "      <td>2018-04-19 09:01:01</td>\n",
       "      <td>car</td>\n",
       "      <td>1.140000</td>\n",
       "      <td>-1.916667</td>\n",
       "      <td>9.034000</td>\n",
       "      <td>2.207333</td>\n",
       "      <td>-1.138000</td>\n",
       "      <td>9.796667</td>\n",
       "      <td>1.588895</td>\n",
       "      <td>-1.420076</td>\n",
       "      <td>9.436495</td>\n",
       "      <td>0.462475</td>\n",
       "      <td>0.295181</td>\n",
       "      <td>0.286873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cace4ec0999436917986b4fa6e9317262c897bc2</td>\n",
       "      <td>2019-05-28 13:12:05</td>\n",
       "      <td>2019-05-28 13:12:10</td>\n",
       "      <td>car</td>\n",
       "      <td>6.880200</td>\n",
       "      <td>3.253137</td>\n",
       "      <td>6.115000</td>\n",
       "      <td>7.112941</td>\n",
       "      <td>3.365510</td>\n",
       "      <td>6.210600</td>\n",
       "      <td>7.039480</td>\n",
       "      <td>3.302962</td>\n",
       "      <td>6.174118</td>\n",
       "      <td>0.095806</td>\n",
       "      <td>0.042367</td>\n",
       "      <td>0.037818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d429974540bfd38c3367fe9f0c8682775ff4fa18</td>\n",
       "      <td>2018-04-19 13:24:36</td>\n",
       "      <td>2018-04-19 13:24:41</td>\n",
       "      <td>car</td>\n",
       "      <td>-0.282800</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>8.404898</td>\n",
       "      <td>1.155000</td>\n",
       "      <td>4.941020</td>\n",
       "      <td>10.076667</td>\n",
       "      <td>0.355344</td>\n",
       "      <td>2.458931</td>\n",
       "      <td>9.147477</td>\n",
       "      <td>0.626277</td>\n",
       "      <td>2.448191</td>\n",
       "      <td>0.647715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user           Start Time  \\\n",
       "0  a526f3566e9c9024dfa7378eb4291d787a09fd37  2018-04-14 08:39:00   \n",
       "1  dd82e3df4bebc74ed6b67877be79e29f401c16a3  2019-05-27 06:11:39   \n",
       "2  d429974540bfd38c3367fe9f0c8682775ff4fa18  2018-04-19 09:00:56   \n",
       "3  cace4ec0999436917986b4fa6e9317262c897bc2  2019-05-28 13:12:05   \n",
       "4  d429974540bfd38c3367fe9f0c8682775ff4fa18  2018-04-19 13:24:36   \n",
       "\n",
       "              End Time Class      xmin      ymin      zmin      xmax  \\\n",
       "0  2018-04-14 08:39:05   car  9.100000 -0.177222  2.914167  9.319474   \n",
       "1  2019-05-27 06:11:44   car -3.102917  0.505208  9.358878 -2.989388   \n",
       "2  2018-04-19 09:01:01   car  1.140000 -1.916667  9.034000  2.207333   \n",
       "3  2019-05-28 13:12:10   car  6.880200  3.253137  6.115000  7.112941   \n",
       "4  2018-04-19 13:24:41   car -0.282800  0.137500  8.404898  1.155000   \n",
       "\n",
       "       ymax       zmax     xmean     ymean     zmean      xstd      ystd  \\\n",
       "0  1.823158   3.394375  9.197301  0.680411  3.147691  0.083206  0.848583   \n",
       "1  0.940101   9.446250 -3.034444  0.772117  9.399396  0.047974  0.190513   \n",
       "2 -1.138000   9.796667  1.588895 -1.420076  9.436495  0.462475  0.295181   \n",
       "3  3.365510   6.210600  7.039480  3.302962  6.174118  0.095806  0.042367   \n",
       "4  4.941020  10.076667  0.355344  2.458931  9.147477  0.626277  2.448191   \n",
       "\n",
       "       zstd  \n",
       "0  0.218015  \n",
       "1  0.037906  \n",
       "2  0.286873  \n",
       "3  0.037818  \n",
       "4  0.647715  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.09999996, -0.17722222,  2.91416665, ...,  0.08320593,\n",
       "         0.84858331,  0.21801462],\n",
       "       [-3.10291667,  0.50520833,  9.35887754, ...,  0.04797357,\n",
       "         0.19051301,  0.03790587],\n",
       "       [ 1.13999999, -1.91666668,  9.03399998, ...,  0.46247537,\n",
       "         0.29518099,  0.28687262],\n",
       "       ...,\n",
       "       [ 0.06486238,  8.62687494,  4.04567566, ...,  0.06309651,\n",
       "         0.24932396,  0.29473452],\n",
       "       [-0.16757576,  8.52562494,  4.03099996, ...,  0.13648608,\n",
       "         0.15416799,  0.12227548],\n",
       "       [-0.0375    ,  8.49538458,  4.18035712, ...,  0.0821184 ,\n",
       "         0.24461005,  0.07151728]], shape=(17372, 9))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Training and test data split: Splitting the dataset into training, validation, and test data set using 60:20:20 split for train: validation: test.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "db.head()\n",
    "\n",
    "#X = db.iloc[:,4:] \n",
    "X = db[['xmin', 'ymin', 'zmin', 'xmean', 'ymean', 'zmean', 'xstd', 'ystd', 'zstd']].values\n",
    "#you can play with using less features to see the impact on the accuracy \n",
    "#X = data[['xmean', 'ymean', 'zmean', 'xstd', 'ystd', 'zstd']].values\n",
    "#X = data[['xstd', 'zstd']].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['car', 'bus', 'walk', 'bike', 'train', 'e-bike'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check which unique classes are available in the dataset (Added myself)\n",
    "y = db['Class']\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "y = db['Class']\n",
    "#label encoding is done as model accepts only numeric values\n",
    "# so strings need to be converted into labels\n",
    "LE = preprocessing.LabelEncoder()\n",
    "LE.fit(y)\n",
    "y = LE.transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike → 0\n",
      "bus → 1\n",
      "car → 2\n",
      "e-bike → 3\n",
      "train → 4\n",
      "walk → 5\n"
     ]
    }
   ],
   "source": [
    "#Which numbers belong to what transport modes (Added myself)\n",
    "for original, encoded in zip(LE.classes_, LE.transform(LE.classes_)):\n",
    "    print(f\"{original} → {encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoints in Training set: 10422\n",
      "Datapoints in validation set: 3475\n",
      "Datapoints in Test set: 3475\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "#splitting dataset into train, validation and test data\n",
    "#De originele dataset bestaat uit features X en labels y.\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 0) #80% training, 20% test\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.25,random_state = 0)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "#Output the number of data points in training, validation, and test dataset.\n",
    "print(\"Datapoints in Training set:\",len(X_train))\n",
    "print(\"Datapoints in validation set:\",len(X_val))\n",
    "print(\"Datapoints in Test set:\",len(X_test))\n",
    "\n",
    "# Convert the y variable into one-hot encoding - basically the true label will be 1 and all others will be assigned to 0\n",
    "def one_hot(y, num_classes):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_train_oh = one_hot(y_train, len(np.unique(y)))\n",
    "y_val_oh = one_hot(y_val, len(np.unique(y)))\n",
    "y_test_oh = one_hot(y_test, len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations is : 3.110968904733291\n",
      "Cost after 100 iterations is : 1.704504385058459\n",
      "Cost after 200 iterations is : 1.4881258922656\n",
      "Cost after 300 iterations is : 1.4244776237987191\n",
      "Cost after 400 iterations is : 1.3901662022443935\n",
      "Cost after 500 iterations is : 1.3674872375758158\n",
      "Cost after 600 iterations is : 1.350805163699869\n",
      "Cost after 700 iterations is : 1.3377300372778589\n",
      "Cost after 800 iterations is : 1.3270795198162486\n",
      "Cost after 900 iterations is : 1.318186723170915\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the Train Dataset 43.31 %\n",
      "Accuracy on the Validation Dataset 42.68 %\n"
     ]
    }
   ],
   "source": [
    "# %% -------------  PART A -------------------\n",
    "#Train NN models to obtain the accuracy on the test data using your training and validation data.\n",
    "#Simple Neural Network from Scratch with One Hidden Layer \n",
    "#Forward and backward propogation are to be implemented in detail \n",
    "\n",
    "\n",
    "#activation functions  Activatiefuncties helpen om niet-lineariteit aan het model toe te voegen en bepalen hoe signalen door het netwerk gaan.\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def softmax(x): #Wordt gebruikt bij output laag\n",
    "    expX = np.exp(x)\n",
    "    return expX/np.sum(expX, axis = 0)\n",
    "\n",
    "#derivatives - activation functions - to be used in backward progation of errors \n",
    "#Deze worden gebruikt in backpropagation om fouten door te geven en het netwerk bij te werken.\n",
    "\n",
    "\n",
    "def derivative_tanh(x):\n",
    "    return (1 - np.power(np.tanh(x), 2))\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.array(x > 0, dtype = np.float32)\n",
    "\n",
    "#n_x number of features aka input variables\n",
    "#n_h number of neurons in the hidden layer \n",
    "#n_y number of classes \n",
    "\n",
    "\n",
    "#definition of parameters (theta) between each input in the input layer, each neuron in the hidden layer, and output in the output layer\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    theta_1 = np.random.randn(n_h, n_x)#*0.1  scaling used for the case of RELU \n",
    "    theta0_1 = np.zeros((n_h, 1))\n",
    "    \n",
    "    theta_2 = np.random.randn(n_y, n_h)#*0.1  scaling used for the case of RELU \n",
    "    theta0_2 = np.zeros((n_y, 1))\n",
    "\n",
    "#theta_1 verbindt de inputlaag met de verborgen laag.\n",
    "#theta_2 verbindt de verborgen laag met de outputlaag.\n",
    "#De biases (theta0_1, theta0_2) worden op nul gezet.\n",
    "    \n",
    "    parameters = {\n",
    "        \"theta_1\" : theta_1,  \n",
    "        \"theta0_1\" : theta0_1, #bias / intercept from the input layer\n",
    "        \"theta_2\" : theta_2,\n",
    "        \"theta0_2\" : theta0_2 #bias / intercept from the hidden layer\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(x, parameters):\n",
    "    \n",
    "    theta_1 = parameters['theta_1']\n",
    "    theta0_1 = parameters['theta0_1']\n",
    "    theta_2 = parameters['theta_2']\n",
    "    theta0_2 = parameters['theta0_2']\n",
    "    \n",
    "    # !!!!!!! IMPLEMENT !!!!!!!!!!!!!!!!\n",
    "    #linear combination of first set of parameters and the inputs \n",
    "    x1 = np.dot(theta_1, x) + theta0_1\n",
    "    xh1 = tanh(x1) #activation function\n",
    "    #linear combination of the second set of parameteres and the output of the hidden layer\n",
    "    x2 = np.dot(theta_2, xh1) + theta0_2 #not sure if this has to be x1 or xh1?\n",
    "    xh2 = softmax(x2) #softmax function at the output layer for the classification task\n",
    "    \n",
    "    forward_cache = {\n",
    "        \"x1\" : x1,\n",
    "        \"xh1\" : xh1,\n",
    "        \"x2\" : x2,\n",
    "        \"xh2\" : xh2\n",
    "    }\n",
    "    \n",
    "    return forward_cache\n",
    "\n",
    "def cost_function(xh2, y):\n",
    "    m = y.shape[1]\n",
    "    #cross-entropy loss (also in your slides - for multiclass classification with softmax)\n",
    "    #y * np.log(xh2) --> Log Likelihood function\n",
    "    # -(1/m)* np.sum --> gemiddelde over alle voorbeelden\n",
    "    cost = -(1/m)*np.sum(y*np.log(xh2))\n",
    "    return cost\n",
    "\n",
    "def backward_prop(x, y, parameters, forward_cache):\n",
    "    \n",
    "    theta_1 = parameters['theta_1']\n",
    "    theta0_1 = parameters['theta0_1']\n",
    "    theta_2 = parameters['theta_2']\n",
    "    theta0_2 = parameters['theta0_2']\n",
    "    \n",
    "    xh1 = forward_cache['xh1']\n",
    "    xh2 = forward_cache['xh2']\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    \n",
    "    dx2 = (xh2 - y)  #output layer with the softmax - partial derivative with respect to x2, this is given to you and have quite some derivations behind \n",
    "    #fout in de output laag\n",
    "\n",
    "    #partial derivative with respect to the second set of parameters based on the error above, aanpassing voor tweede set parameters.\n",
    "    dtheta_2 = (1/m)*np.dot(dx2, xh1.T)\n",
    "    dtheta0_2 = (1/m)*np.sum(dx2, axis = 1, keepdims = True)\n",
    "    \n",
    "    #error propagated to the hidden layer \n",
    "    dx1 = (1/m)*np.dot(theta_2.T, dx2)*derivative_tanh(xh1)  #needs to be tailored to the chosen activation function at the hidden layer\n",
    "    \n",
    "    # !!!!!!! IMPLEMENT !!!!!!!!!!!!!!!!\n",
    "    #partial derivative with respect to the first set of parameters based on the error above. \n",
    "    #Theta wordt dus opnieuw bepaald, om nauwkeurigere berekening uit te kunnen voeren.\n",
    "    dtheta_1 = (1/m) * np.dot(dx1, x.T)\n",
    "    dtheta0_1 = (1/m)*np.sum(dx1, axis = 1, keepdims = True)\n",
    "    \n",
    "    gradients = {\n",
    "        \"dtheta_1\" : dtheta_1,\n",
    "        \"dtheta0_1\" : dtheta0_1,\n",
    "        \"dtheta_2\" : dtheta_2,\n",
    "        \"dtheta0_2\" : dtheta0_2\n",
    "    }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \n",
    "    theta_1 = parameters['theta_1']\n",
    "    theta0_1 = parameters['theta0_1']\n",
    "    theta_2 = parameters['theta_2']\n",
    "    theta0_2 = parameters['theta0_2']\n",
    "    \n",
    "    dtheta_1 = gradients['dtheta_1']\n",
    "    dtheta0_1 = gradients['dtheta0_1']\n",
    "    dtheta_2 = gradients['dtheta_2']\n",
    "    dtheta0_2 = gradients['dtheta0_2']\n",
    "    \n",
    "    #update of the first and second set of parameters based on the partial derivatives (gradients) and the learning rate\n",
    "    theta_1 = theta_1 - learning_rate*dtheta_1\n",
    "    theta0_1 = theta0_1 - learning_rate*dtheta0_1\n",
    "    theta_2 = theta_2 - learning_rate*dtheta_2\n",
    "    theta0_2 = theta0_2 - learning_rate*dtheta0_2\n",
    "    \n",
    "    parameters = {\n",
    "        \"theta_1\" : theta_1,\n",
    "        \"theta0_1\" : theta0_1,\n",
    "        \"theta_2\" : theta_2,\n",
    "        \"theta0_2\" : theta0_2\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def NN_singleHiddenLayer(x, y, n_h, learning_rate, iterations):\n",
    "    \n",
    "    n_x = x.shape[0]\n",
    "    n_y = y.shape[0]\n",
    "    \n",
    "    cost_list = []\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        forward_cache = forward_propagation(x, parameters)\n",
    "        \n",
    "        cost = cost_function(forward_cache['xh2'], y)\n",
    "        \n",
    "        gradients = backward_prop(x, y, parameters, forward_cache)\n",
    "        \n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        if(i%(iterations/10) == 0):\n",
    "            print(\"Cost after\", i, \"iterations is :\", cost)\n",
    "        \n",
    "    return parameters, cost_list\n",
    "\n",
    "iterations = 1000\n",
    "n_h = 10\n",
    "learning_rate = 0.05\n",
    "Parameters, Cost_list = NN_singleHiddenLayer(X_train.T, y_train_oh.T, n_h = n_h, learning_rate = learning_rate, iterations = iterations)\n",
    "\n",
    "t = np.arange(0, iterations)\n",
    "plt.plot(t, Cost_list)\n",
    "plt.show()\n",
    "\n",
    "#accuracy function based on a test/validation dataset \n",
    "def accuracy(inp, labels, parameters):\n",
    "    forward_cache = forward_propagation(inp, parameters)\n",
    "    y_out = forward_cache['xh2']   # containes propabilities with shape(6, 1)\n",
    "    y_out = np.argmax(y_out, 0)  # 0 represents row wise \n",
    "    labels = np.argmax(labels, 0)\n",
    "    acc = np.mean(y_out == labels)*100\n",
    "    \n",
    "    return acc\n",
    "\n",
    "print(\"Accuracy on the Train Dataset\", round(accuracy(X_train.T, y_train_oh.T, Parameters),2), \"%\")\n",
    "print(\"Accuracy on the Validation Dataset\", round(accuracy(X_val.T, y_val_oh.T, Parameters), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Logistic Regression: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.05      0.08       444\n",
      "           1       0.46      0.40      0.43       968\n",
      "           2       0.35      0.48      0.41      1025\n",
      "           3       0.33      0.17      0.22         6\n",
      "           4       0.60      0.69      0.64       109\n",
      "           5       0.54      0.63      0.58       923\n",
      "\n",
      "    accuracy                           0.45      3475\n",
      "   macro avg       0.46      0.40      0.39      3475\n",
      "weighted avg       0.45      0.45      0.43      3475\n",
      "\n",
      "Accuracy of logistic regression on the initial data is:  0.44805755395683455\n",
      "For Neural Network: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.45      0.55       444\n",
      "           1       0.68      0.74      0.71       968\n",
      "           2       0.71      0.67      0.69      1025\n",
      "           3       0.40      0.33      0.36         6\n",
      "           4       0.76      0.81      0.78       109\n",
      "           5       0.73      0.84      0.78       923\n",
      "\n",
      "    accuracy                           0.71      3475\n",
      "   macro avg       0.67      0.64      0.65      3475\n",
      "weighted avg       0.71      0.71      0.70      3475\n",
      "\n",
      "Accuracy of NN on the initial data is:  0.7090647482014388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hilde\\Anaconda\\envs\\ME44312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% -------------  PART B -------------------\n",
    "#Now Train NN models using SKLEARN \n",
    "#Logistic regression (that can handle multi class classification) is provided for you. \n",
    "\n",
    "#Missing parts that you need to implement to answer the question are indicated below (with \"Implement!\")\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_logreg = LogisticRegression(random_state=0,max_iter = 200).fit(X_train,y_train)\n",
    "\n",
    "# !!!!!!! IMPLEMENT !!!!!!!!!!!!!!!!\n",
    "# Train a Neural Network\n",
    "train_nn = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=0, max_iter=500, learning_rate_init= 0.0001)\n",
    "train_nn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "pred_logreg = train_logreg.predict(X_val)\n",
    "print(\"For Logistic Regression: \")\n",
    "print(classification_report(y_val, pred_logreg))\n",
    "print (\"Accuracy of logistic regression on the initial data is: \",accuracy_score(pred_logreg,y_val))\n",
    "\n",
    "# !!!!!!! IMPLEMENT !!!!!!!!!!!!!!!!\n",
    "#Predict based on the trained Neural Network using the validation data\n",
    "#NOTE: You should reach a NN which has a better accuracy than the logistic regression, if  not revisit the specification of your NN\n",
    "\n",
    "pred_nn = train_nn.predict(X_val)\n",
    "print(\"For Neural Network: \")\n",
    "print(classification_report(y_val, pred_nn))\n",
    "print (\"Accuracy of NN on the initial data is: \",accuracy_score(pred_nn,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% -------------  PART C -------------------\n",
    "#We can also extract features such as minute, hour and day from timestamp column as it was not used till now and try to improve the above accuracies\n",
    "\n",
    "#Feature Extraction from Timestamp column\n",
    "db.head()\n",
    "db['Start Time'] = db['Start Time'].astype('datetime64[ns]')\n",
    "db['hour'] = db['Start Time'].dt.hour\n",
    "db['minute'] = db['Start Time'].dt.minute\n",
    "db['day'] = db['Start Time'].dt.day\n",
    "db.head()\n",
    "\n",
    "#Again training the classifiers\n",
    "X = db.iloc[:,4:]\n",
    "y = db['Class']\n",
    "#label encoding is done as model accepts only numeric values\n",
    "# so strings need to be converted into labels\n",
    "LE = preprocessing.LabelEncoder()\n",
    "LE.fit(y)\n",
    "y = LE.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['xmin', 'ymin', 'zmin', 'xmax', 'ymax', 'zmax', 'xmean', 'ymean',\n",
      "       'zmean', 'xstd', 'ystd', 'zstd', 'hour', 'minute', 'day'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)  # Hiermee controleer je of 'hour', 'minute' en 'day' aanwezig zijn, dus ja die zijn toegevoegd aan de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataset into train, validation and test data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 0)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.25,random_state = 0)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the y variable into one-hot encoding - basically the true label will be 1 and all others will be assigned to 0\n",
    "def one_hot(y, num_classes):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_train_oh = one_hot(y_train, len(np.unique(y)))\n",
    "y_val_oh = one_hot(y_val, len(np.unique(y)))\n",
    "y_test_oh = one_hot(y_test, len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 ... 2 1 2]\n",
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Logistic Regression: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.04      0.07       444\n",
      "           1       0.51      0.54      0.52       968\n",
      "           2       0.46      0.52      0.49      1025\n",
      "           3       0.29      0.33      0.31         6\n",
      "           4       0.60      0.64      0.62       109\n",
      "           5       0.54      0.66      0.59       923\n",
      "\n",
      "    accuracy                           0.50      3475\n",
      "   macro avg       0.46      0.45      0.43      3475\n",
      "weighted avg       0.49      0.50      0.47      3475\n",
      "\n",
      "Accuracy of logistic regression on the extended data is:  0.5018705035971223\n",
      "For Neural Network: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86       444\n",
      "           1       0.92      0.92      0.92       968\n",
      "           2       0.91      0.88      0.89      1025\n",
      "           3       0.71      0.83      0.77         6\n",
      "           4       0.96      0.98      0.97       109\n",
      "           5       0.92      0.95      0.93       923\n",
      "\n",
      "    accuracy                           0.91      3475\n",
      "   macro avg       0.88      0.90      0.89      3475\n",
      "weighted avg       0.91      0.91      0.91      3475\n",
      "\n",
      "Accuracy of NN on the extended data is:  0.9096402877697841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hilde\\Anaconda\\envs\\ME44312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "train_logreg2 = LogisticRegression(random_state=0,max_iter = 200).fit(X_train,y_train)\n",
    "\n",
    "# !!!!!!! IMPLEMENT !!!!!!!!!!!!!!!!\n",
    "#Train a NN with more features as explained above \n",
    "train_nn2 = MLPClassifier(hidden_layer_sizes=(30,30), activation='relu', solver='lbfgs', random_state=0, max_iter=1000)\n",
    "train_nn2.fit(X_train, y_train)\n",
    "\n",
    "pred_logreg2 = train_logreg2.predict(X_val)\n",
    "print(\"For Logistic Regression: \")\n",
    "print(classification_report(y_val, pred_logreg2))\n",
    "print (\"Accuracy of logistic regression on the extended data is: \",accuracy_score(pred_logreg2,y_val))\n",
    "\n",
    "# !!!!!!! IMPLEMENT !!!!!!!!!!!!!!!!\n",
    "# Predict based on the trained NN using the validation data\n",
    "#NOTE: Again you should reach a NN which has a better accuracy than the logistic regression, if  not revisit the specification of your NN\n",
    "\n",
    "pred_nn2 = train_nn2.predict(X_val)\n",
    "print(\"For Neural Network: \")\n",
    "print(classification_report(y_val, pred_nn2))\n",
    "print (\"Accuracy of NN on the extended data is: \",accuracy_score(pred_nn2,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the chosen Classifier is:  0.7741007194244605\n"
     ]
    }
   ],
   "source": [
    "# %% -------------  PART D -------------------\n",
    "#Accuracy of the models should increase by using additional features\n",
    "#Pick the one with the highest accuracy and apply it to the test data. \n",
    "\n",
    "# !!!!!!! IMPLEMENT !!!!!!!!!!!!!!!!\n",
    "#NOTE you should reach an accuracy of at least 80% so revisit your models if you cannot reach that. \n",
    "\n",
    "final_res = train_nn2.predict(X_test)\n",
    "print (\"Accuracy of the chosen Classifier is: \",accuracy_score(final_res,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ME44312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
